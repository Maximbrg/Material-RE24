{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Imports"
      ],
      "metadata": {
        "id": "TODY6V_Z-syw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IEJfX6N3pVCu"
      },
      "outputs": [],
      "source": [
        "!python -m spacy download en_core_web_lg # NEED TO RESTART THE KERNAL AFTER DOWNLOADING"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade scikit-learn"
      ],
      "metadata": {
        "id": "OXMm5WFgdbkf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# You need a path to word2vec_model; e.g: https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit?resourcekey=0-wjGZdNAUop6WykTtMip30g\n",
        "model_file_path = 'word2vec_model.bin'\n",
        "\n",
        "USER_STORIES_NAMES = ['g13-planningpoker.txt', 'g12-camperplus.txt', 'g14-datahub.txt', 'g28-zooniverse.txt', 'g04-recycling.txt',\n",
        "'g08-frictionless.txt', 'g24-unibath.txt','g02-federalspending.txt','g03-loudoun.txt']\n",
        "\n",
        "OUTPUT_PATH = 'class_dataset.csv'\n",
        "csv_file_path = 'gold-standard_classes.csv'  # Replace with the path to your CSV file that cotains one column named \"x\" and the values of this column are the classes from the gold standard\n",
        "user_story_dataset = 'supermarket.txt' # user story dataset"
      ],
      "metadata": {
        "id": "0lAu3zx0W7bf"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6M-_CZqpXVpY",
        "outputId": "80acae24-15f5-41be-933f-dea3649ddc6e"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UvxY9qPbqkiz"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "from scipy import stats as st\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "import spacy\n",
        "from nltk.stem.porter import *\n",
        "import gensim\n",
        "from gensim import models\n",
        "from gensim.models import Word2Vec, KeyedVectors\n",
        "import pickle as pickle\n",
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import editdistance\n",
        "from tqdm import tqdm\n",
        "import nltk\n",
        "\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nlp = spacy.load('en_core_web_lg')\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.cluster import KMeans\n",
        "!pip install plotly==4.14.3\n",
        "import plotly.express as px\n",
        "import gensim.downloader as api\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')\n",
        "nltk.download('porter_test')\n",
        "\n",
        "import csv\n",
        "import nltk\n",
        "from nltk import word_tokenize, pos_tag\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Download NLTK data (you can skip this if already downloaded)\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "from gensim.models import KeyedVectors\n",
        "word2vec = KeyedVectors.load(model_file_path)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_compound_nouns(text):\n",
        "    tokens = word_tokenize(text)\n",
        "    tagged_tokens = pos_tag(tokens)\n",
        "\n",
        "    compound_nouns = []\n",
        "    current_compound = []\n",
        "\n",
        "    for word, pos in tagged_tokens:\n",
        "        if pos.startswith('NN') and word.lower() not in stopwords.words('english'):\n",
        "            current_compound.append(word)\n",
        "        else:\n",
        "            if len(current_compound) > 1:\n",
        "                compound_nouns.append(' '.join(current_compound))\n",
        "            current_compound = []\n",
        "\n",
        "    return compound_nouns\n",
        "\n",
        "def extract_nouns(text):\n",
        "    tokens = word_tokenize(text)\n",
        "    tagged_tokens = pos_tag(tokens)\n",
        "\n",
        "    # Filter out stopwords and get only nouns\n",
        "    nouns = [word for word, pos in tagged_tokens if pos.startswith('NN') and word.lower() not in stopwords.words('english')]\n",
        "\n",
        "    return nouns\n",
        "\n",
        "def read_text_file(file_path):\n",
        "    with open(file_path, 'r', encoding='utf-8') as file:\n",
        "        return file.read()\n",
        "\n",
        "def write_to_csv(file_path, headers, data):\n",
        "    with open(file_path, 'w', encoding='utf-8', newline='') as csv_file:\n",
        "        csv_writer = csv.writer(csv_file)\n",
        "        csv_writer.writerow(headers)\n",
        "        csv_writer.writerows(data)\n"
      ],
      "metadata": {
        "id": "76bIP_gETJpU"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kKrcf_qr-5y4"
      },
      "source": [
        "### Create the target (0/1) coulmn"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Load CSV into DataFrame\n",
        "\n",
        "df = pd.read_csv(csv_file_path)\n",
        "\n",
        "# Display the original DataFrame\n",
        "print(\"Original DataFrame:\")\n",
        "print(df)\n",
        "\n",
        "# Choose the column to perform stemming on\n",
        "column_name = 'x'  # Replace with the name of your column\n",
        "selected_column = df[column_name]\n",
        "\n",
        "# Create a Porter Stemmer instance\n",
        "porter_stemmer = PorterStemmer()\n",
        "\n",
        "# Define a function to perform stemming on a single text\n",
        "def perform_stemming(text):\n",
        "    words = word_tokenize(text)\n",
        "    stemmed_words = [porter_stemmer.stem(word) for word in words]\n",
        "    return ' '.join(stemmed_words)\n",
        "\n",
        "# Apply stemming to the selected column\n",
        "df[column_name] = selected_column.apply(perform_stemming)\n",
        "df_no_duplicates = df.drop_duplicates(subset=column_name)\n",
        "\n",
        "# Display the DataFrame after stemming\n",
        "print(\"\\nDataFrame after stemming:\")\n",
        "print(df_no_duplicates)\n",
        "df_no_duplicates.to_csv('new.csv')"
      ],
      "metadata": {
        "id": "Dg-VD4RATQyQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "entites_calculate = df_no_duplicates['x'].tolist()\n",
        "sorted(entites_calculate)"
      ],
      "metadata": {
        "id": "YwmYqHNimO4R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Stop words"
      ],
      "metadata": {
        "id": "AOzA4c4m_eyB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "jyV8A8NznuVu"
      },
      "outputs": [],
      "source": [
        "stop_words = set(stopwords.words('english')) # getting the english stopwords set from nltk\n",
        "stop_words.add(',')\n",
        "stop_words.add('.')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Add new entity"
      ],
      "metadata": {
        "id": "of2mkHDe_UrT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "K5KmAiCTkUKY"
      },
      "outputs": [],
      "source": [
        "def add_entity(word, word_org, pos, dep, dict_entites):\n",
        "\n",
        "  \"\"\"\n",
        "  Params:\n",
        "  word (str): The word after stemming has been applied\n",
        "  word_org (str): The word before stemming has been applied (the original)\n",
        "  pos (str): Part-of-Speech ('POS') of the word\n",
        "  dep (str): The dependency label of the word\n",
        "  dict_entities (dict): the current dictionary of entites\n",
        "\n",
        "  Returns:\n",
        "  to_enter (dict): The dict that matches the current word after update\n",
        "  \"\"\"\n",
        "  to_enter = {'original_word': word_org, 'count': 1, 'noun': 0, 'subject': 0, 'compound': 0, 'gerund': 0, 'target': 0}\n",
        "  if word in dict_entites.keys():\n",
        "    to_enter = dict_entites[word]\n",
        "    to_enter['count'] += 1\n",
        "  if word_org in entites_calculate:\n",
        "\n",
        "    to_enter['target'] = 1\n",
        "\n",
        "\n",
        "  for entity in entites_calculate:\n",
        "      if entity == word_org or entity == word:\n",
        "        to_enter['target'] = 1\n",
        "\n",
        "\n",
        "  # Based on the Part of Speech and dependency of the word we increase the\n",
        "  # relevant counter in \"to_enter\" by one.\n",
        "  if 'NN' in pos:\n",
        "    to_enter['noun'] += 1\n",
        "\n",
        "  if 'VBG' in pos:\n",
        "    to_enter['gerund'] += 1\n",
        "\n",
        "  if 'nsubj' == dep:\n",
        "    to_enter['subject'] += 1\n",
        "\n",
        "  if 'compound' == dep:\n",
        "    to_enter['compound'] += 1\n",
        "\n",
        "  return to_enter\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loop over the stories and create the entities"
      ],
      "metadata": {
        "id": "lqUeL-r8ADTb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "visited = set()\n",
        "dict_entites = {}\n",
        "\n",
        "with open(user_story_dataset,encoding='cp1252') as f:\n",
        "  lines = f.readlines() # List of all User stories seperated by '\\n'.\n",
        "\n",
        "for line in lines:\n",
        "  doc = nlp(line)\n",
        "  tokenized_doc = []\n",
        "  tokenized_doc = [token for token in doc if token.text.lower() not in stop_words]\n",
        "  tokenized_doc = [token for token in tokenized_doc if len(token.text) > 1]\n",
        "\n",
        "\n",
        "  tokens = []     # Tokens is a list which will hold every token in each doc.\n",
        "\n",
        "  for token in tokenized_doc:\n",
        "    tokens.append(token)\n",
        "    word = stemmer.stem(token.text).lower()    # Stemmed version of the current word (token).\n",
        "    word_org = token.text                      # The original.\n",
        "    if word_org in ['estimator', 'estimators']:                # TODO: fix case where estimator's stemming doesnt work correctly\n",
        "      word = 'estimator'\n",
        "\n",
        "    pos = token.tag_\n",
        "    if 'NN' in pos:          # If the word's PoS (part of speech) is a noun ('NN'),\n",
        "      visited.add(word)       # add it to visited (set)\n",
        "\n",
        "  for index in range(len(tokens) - 2):\n",
        "    # Iterate through the tokens, 3 at a time, note that this way the last two\n",
        "    # words gets over looked and should be treated after the loop ends.\n",
        "\n",
        "    word_org_1 = tokens[index].text            # Three original words\n",
        "    word_org_2 = tokens[index + 1].text\n",
        "    word_org_3 = tokens[index + 2].text\n",
        "\n",
        "    word_1 = stemmer.stem(word_org_1).lower()  # Three matching stemmed words\n",
        "    word_2 = stemmer.stem(word_org_2).lower()\n",
        "    word_3 = stemmer.stem(word_org_3).lower()\n",
        "\n",
        "\n",
        "    if word_org_1 in ['estimator', 'estimators']:  # Treating the case of estimator specificly\n",
        "      word_1 = 'estimator'\n",
        "    if word_org_2 in ['estimator', 'estimators']:\n",
        "      word_2 = 'estimator'\n",
        "    if word_org_3 in ['estimator', 'estimators']:\n",
        "      word_3 = 'estimator'\n",
        "\n",
        "    pos_1 = tokens[index].tag_       # Get each token's PoS (part of speech)\n",
        "    dep_1 = tokens[index].dep_        # Get each token's dependency label\n",
        "    pos_2 = tokens[index + 1].tag_\n",
        "    dep_2 = tokens[index + 1].dep_\n",
        "    pos_3 = tokens[index + 2].tag_\n",
        "    dep_3 = tokens[index + 2].dep_\n",
        "\n",
        "    if word_1 in visited:  # Meaning word_1 is a stemmed noun which we visited already\n",
        "\n",
        "\n",
        "    #### USAGE OF SPECIFIC CONDITIONS DERIVED FROM ARTICLE file:///C:/Users/X260/Downloads/s00766-017-0270-1.pdf:\n",
        "\n",
        "      dict_entites[word_1] = add_entity(word_1, word_org_1,\n",
        "                                        pos_1, dep_1, dict_entites)  # Update dict_entites\n",
        "\n",
        "      if 'NN' in pos_2: # If pos_2 is a noun:\n",
        "\n",
        "        if 'VBG' in pos_1: # If the part of speech of word 1 is a verb:\n",
        "          pos_1 = 'VBG'\n",
        "        if 'nsubj' in dep_2: # If the dependency of word 2 is a nominal subject:\n",
        "          dep_1 = 'nsubj'\n",
        "        if 'compound' in dep_2: # If the dependency of word 2 is a compound noun:\n",
        "          dep_1 = 'compound'\n",
        "\n",
        "        dict_entites[word_1 + ' ' + word_2] = add_entity(word_1 + ' ' + word_2,\n",
        "                    word_org_1 + ' ' + word_org_2, pos_1, dep_1, dict_entites) # update the dict with the new compound noun\n",
        "\n",
        "\n",
        "      if 'NN' in pos_3: # If pos_3 is a noun:\n",
        "\n",
        "        if 'VBG' in pos_1 or 'VBG' in pos_3: # if either pos_1 or pos_3 are gerund verbs:\n",
        "          pos_1 = 'VBG'\n",
        "        if 'nsubj' in dep_3: # if the dependency of word 2 is a nominal subject:\n",
        "          dep_1 = 'nsubj'\n",
        "        if 'compound' in dep_3: # if the dependency of word 3 is a compound noun:\n",
        "          dep_1 = 'compound'\n",
        "\n",
        "        dict_entites[word_1 + ' ' + word_2 + ' ' + word_3] = add_entity(word_1 + ' ' + word_2 + ' ' + word_3, word_org_1 + ' ' +\n",
        "                           word_org_2 + ' ' + word_org_3, pos_1, dep_1, dict_entites) # update the dict with the new compound noun\n",
        "\n",
        "\n",
        "\n",
        "  # Treating the last words:\n",
        "  try:\n",
        "    last_word_org_1 = tokens[-2].text\n",
        "    last_word_org_2 = tokens[-1].text\n",
        "    last_word_1 = stemmer.stem(last_word_org_1).lower()\n",
        "    last_word_2 = stemmer.stem(last_word_org_2).lower()\n",
        "    if last_word_1 in visited:\n",
        "      if last_word_org_1 in ['estimator', 'estimators']:\n",
        "        last_word_1 = 'estimator'\n",
        "      if last_word_org_2 in ['estimator', 'estimators']:\n",
        "        last_word_2 = 'estimator'\n",
        "      last_pos_1 = tokens[-2].tag_\n",
        "      last_dep_1 = tokens[-2].dep_\n",
        "      last_pos_2 = tokens[-1].tag_\n",
        "      last_dep_2 = tokens[-1].dep_\n",
        "      dict_entites[last_word_1] = add_entity(last_word_1, last_word_org_1,\n",
        "                                          last_pos_1, last_dep_1, dict_entites)\n",
        "      if compound_check_pair(last_pos_1, last_pos_2):\n",
        "        dict_entites[last_word_1 + ' ' + last_word_2] = add_entity(last_word_1 + ' ' + last_word_2,\n",
        "                        last_word_org_1 + ' ' + last_word_org_2, last_pos_1, last_dep_1, dict_entites)\n",
        "\n",
        "    if last_word_2 in visited:\n",
        "      dict_entites[last_word_2] = add_entity(last_word_2, last_word_org_2,\n",
        "                                        last_pos_2, last_dep_2, dict_entites)\n",
        "  except:\n",
        "    pass"
      ],
      "metadata": {
        "id": "G9ic4jkgVuQi"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "rCjLI4s7Kpaq"
      },
      "outputs": [],
      "source": [
        "def compound_check_pair(pos_1, pos_2):\n",
        "  # checks if two words are likely a compound noun by checking thier PoSs.\n",
        "  # returns True or False accordingly.\n",
        "  if 'NN' in pos_1:\n",
        "      if 'NN' in pos_2 or 'VBG' in pos_2 or 'IN' in pos_2 or 'JJ' in pos_2:\n",
        "        return True\n",
        "\n",
        "      elif 'NN' in pos_2:\n",
        "        if 'VBG' in pos_1 or 'IN' in pos_1 or 'JJ' in pos_1:\n",
        "          return True\n",
        "\n",
        "      return False\n",
        "\n",
        "\n",
        "def compound_check_triplet(pos_1, pos_2, pos_3):\n",
        "  # checks if three words are likely a compound noun by checking thier PoSs.\n",
        "  # returns True or False accordingly.\n",
        "  if 'NN' in pos_1 and 'IN' in pos_2 and 'NN' in pos_3:\n",
        "    return True\n",
        "  return False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U7vEmPiel3KN"
      },
      "source": [
        "### N-Grams:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "vFZZOfU8l2pl"
      },
      "outputs": [],
      "source": [
        "with open(user_story_dataset,encoding='cp1252') as f:\n",
        "  lines = f.readlines() # List of all User stories seperated by '\\n'.\n",
        "\n",
        "# Initialize a dictionary to store the n-grams and their counts\n",
        "ngram_counts = {}\n",
        "\n",
        "for line in lines:\n",
        "  words = line.split()\n",
        "  tokenized_doc = [token for token in words if token.lower() not in stop_words]\n",
        "  # Loop over each set of 3 adjacent words and count the n-grams\n",
        "  for i in range(len(tokenized_doc) - 3):\n",
        "    ngram = tuple(tokenized_doc[i : i + 4])\n",
        "    if ngram in ngram_counts:\n",
        "        ngram_counts[ngram] += 1\n",
        "    else:\n",
        "        ngram_counts[ngram] = 1\n",
        "\n",
        "frequent_ngrams = [k for k, v in ngram_counts.items() if v > 1]\n",
        "frequent_words_in_ngrams = set()\n",
        "for ngram in frequent_ngrams:\n",
        "  for word in ngram:\n",
        "    frequent_words_in_ngrams.add(word)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### DF creating"
      ],
      "metadata": {
        "id": "PwP0ZxnxAbin"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rDiK3Y9Yb2nB",
        "outputId": "32e900c5-26b6-4e36-f804-d2dee16583cf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 182/182 [00:00<00:00, 91322.33it/s]\n"
          ]
        }
      ],
      "source": [
        "# setting up the features per word\n",
        "data = {'word': [], 'original word': [], 'count': [], 'noun': [], 'subject': [], 'compound': [],\n",
        "        'gerund': [],'part_of_frequent_ngram':[],'target': []}\n",
        "i=0\n",
        "for key in tqdm(dict_entites.keys()):\n",
        "\n",
        "  data['word'].append(key)\n",
        "  data['original word'].append(dict_entites[key]['original_word'])\n",
        "  data['count'].append(dict_entites[key]['count'])\n",
        "  data['noun'].append(dict_entites[key]['noun'])\n",
        "  data['subject'].append(dict_entites[key]['subject'])\n",
        "  data['compound'].append(dict_entites[key]['compound'])\n",
        "  data['gerund'].append(dict_entites[key]['gerund'])\n",
        "  if key in frequent_words_in_ngrams:\n",
        "    data['part_of_frequent_ngram'].append(1)\n",
        "  else:\n",
        "    data['part_of_frequent_ngram'].append(0)\n",
        "\n",
        "  data['target'].append(dict_entites[key]['target'])\n",
        "  i+=1\n",
        "df = pd.DataFrame(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "v0XezNF9ZLBX"
      },
      "outputs": [],
      "source": [
        "# code for adding to an existing df 3 new features:\n",
        "# User_Role: +1 for each time the word appeared in the user role section of the story (group 1)\n",
        "# Action: +1 for each time the word appeared in the Action section of the story (group 2)\n",
        "# Benefit: +1 for each time the word appeared in the Benefit section of the story (group 3)\n",
        "\n",
        "frequency_dict = {}\n",
        "\n",
        "pattern = r\"As (?:an? )?(.+?), I want (.+?), so that (.+)\"\n",
        "for line in lines:\n",
        "\n",
        "# match the pattern to the user story\n",
        "  match = re.match(pattern, line)\n",
        "  if match is not None:\n",
        "\n",
        "    tokenized_user_role = [stemmer.stem(token.text).lower() for token in nlp(match.group(1)) if token.text.lower() not in stop_words]\n",
        "    tokenized_action = [stemmer.stem(token.text).lower() for token in nlp(match.group(2)) if token.text.lower() not in stop_words]\n",
        "    tokenized_benefit = [stemmer.stem(token.text).lower() for token in nlp(match.group(3)) if token.text.lower() not in stop_words]\n",
        "    for word in tokenized_user_role:\n",
        "      if word not in frequency_dict:\n",
        "        frequency_dict[word] = [1,0,0]\n",
        "      else:\n",
        "        frequency_dict[word][0] += 1\n",
        "\n",
        "    for word in tokenized_action:\n",
        "      if word not in frequency_dict:\n",
        "        frequency_dict[word] = [0,1,0]\n",
        "      else:\n",
        "        frequency_dict[word][1] += 1\n",
        "\n",
        "    for word in tokenized_benefit :\n",
        "      if word not in frequency_dict:\n",
        "        frequency_dict[word] = [0,0,1]\n",
        "      else:\n",
        "        frequency_dict[word][2] += 1\n",
        "\n",
        "\n",
        "frequency_dict = pd.DataFrame(frequency_dict).transpose().rename(columns={0: \"User Role\",1: \"Action\",2: \"Benefit\"}).rename_axis('word')\n",
        "\n",
        "# merge the dataframes based on the 'word' column\n",
        "df_merged = pd.merge(df, frequency_dict, on='word', how='left')\n",
        "\n",
        "# group by 'word' and sum the values of the 3 new columns\n",
        "df = df_merged.groupby('word').agg({'original word':'first',\n",
        "                                           'count': 'first',\n",
        "                                           'noun': 'first',\n",
        "                                           'subject': 'first',\n",
        "                                           'compound': 'first',\n",
        "                                           'gerund': 'first',\n",
        "                                           'part_of_frequent_ngram': 'first',\n",
        "                                           'User Role': 'sum',\n",
        "                                           'Action': 'sum',\n",
        "                                           'Benefit': 'sum',\n",
        "                                           'target':'first'}).reset_index()\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Standardize"
      ],
      "metadata": {
        "id": "y1uQ7LW5AjNu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "rBfy19XWvmeH"
      },
      "outputs": [],
      "source": [
        "# standardize the data\n",
        "# df = pd.read_csv('/content/grocery.csv')\n",
        "# Store original column names of DataFrame df\n",
        "org_columns = df.columns\n",
        "\n",
        "# Select a subset of columns from DataFrame df using indexing and store in a new variable\n",
        "columns = df.columns[2:16]\n",
        "scaler = MinMaxScaler()\n",
        "columns_to_scale = columns\n",
        "\n",
        "df[columns_to_scale] = scaler.fit_transform(df[columns_to_scale])\n",
        "\n",
        "# Reorder the columns of the DataFrame back to their original positions\n",
        "df = df[org_columns]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "td8G_2SgC6rD",
        "outputId": "1ae344be-9960-4d8d-8ff1-43ba8df0431a"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                       word             original word     count      noun  \\\n",
              "0                   anomali                 anomalies  0.018182  0.035714   \n",
              "1                    answer                   answers  0.000000  0.017857   \n",
              "2    answer urgent question  answers urgent questions  0.000000  0.017857   \n",
              "3                       app                       app  0.018182  0.035714   \n",
              "4                    applic               application  0.000000  0.017857   \n",
              "..                      ...                       ...       ...       ...   \n",
              "177               time slot                 time slot  0.000000  0.017857   \n",
              "178      time slot deliveri        time slot delivery  0.000000  0.017857   \n",
              "179                transpar              transparency  0.000000  0.017857   \n",
              "180       transpar deliveri     transparency delivery  0.000000  0.017857   \n",
              "181                wishlist                  wishlist  0.000000  0.017857   \n",
              "\n",
              "     subject  compound  gerund  part_of_frequent_ngram  User Role  Action  \\\n",
              "0       0.00      0.00     0.0                     0.0        0.0    0.00   \n",
              "1       0.00      0.00     0.0                     0.0        0.0    0.00   \n",
              "2       0.00      0.00     0.0                     0.0        0.0    0.00   \n",
              "3       0.00      0.00     0.0                     0.0        0.0    0.00   \n",
              "4       0.00      0.00     0.0                     0.0        0.0    0.05   \n",
              "..       ...       ...     ...                     ...        ...     ...   \n",
              "177     0.00      0.25     0.0                     0.0        0.0    0.00   \n",
              "178     0.00      0.25     0.0                     0.0        0.0    0.00   \n",
              "179     0.00      0.00     0.0                     0.0        0.0    0.00   \n",
              "180     0.25      0.00     0.0                     0.0        0.0    0.00   \n",
              "181     0.00      0.00     0.0                     0.0        0.0    0.05   \n",
              "\n",
              "      Benefit  target  \n",
              "0    0.142857     0.0  \n",
              "1    0.071429     0.0  \n",
              "2    0.000000     0.0  \n",
              "3    0.142857     0.0  \n",
              "4    0.000000     0.0  \n",
              "..        ...     ...  \n",
              "177  0.000000     0.0  \n",
              "178  0.000000     0.0  \n",
              "179  0.000000     0.0  \n",
              "180  0.000000     0.0  \n",
              "181  0.000000     0.0  \n",
              "\n",
              "[182 rows x 12 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-77160220-716c-41d8-86e6-60e3ddca7208\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>word</th>\n",
              "      <th>original word</th>\n",
              "      <th>count</th>\n",
              "      <th>noun</th>\n",
              "      <th>subject</th>\n",
              "      <th>compound</th>\n",
              "      <th>gerund</th>\n",
              "      <th>part_of_frequent_ngram</th>\n",
              "      <th>User Role</th>\n",
              "      <th>Action</th>\n",
              "      <th>Benefit</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>anomali</td>\n",
              "      <td>anomalies</td>\n",
              "      <td>0.018182</td>\n",
              "      <td>0.035714</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.142857</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>answer</td>\n",
              "      <td>answers</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.017857</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.071429</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>answer urgent question</td>\n",
              "      <td>answers urgent questions</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.017857</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>app</td>\n",
              "      <td>app</td>\n",
              "      <td>0.018182</td>\n",
              "      <td>0.035714</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.142857</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>applic</td>\n",
              "      <td>application</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.017857</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.05</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>177</th>\n",
              "      <td>time slot</td>\n",
              "      <td>time slot</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.017857</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>178</th>\n",
              "      <td>time slot deliveri</td>\n",
              "      <td>time slot delivery</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.017857</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>179</th>\n",
              "      <td>transpar</td>\n",
              "      <td>transparency</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.017857</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>180</th>\n",
              "      <td>transpar deliveri</td>\n",
              "      <td>transparency delivery</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.017857</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>181</th>\n",
              "      <td>wishlist</td>\n",
              "      <td>wishlist</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.017857</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.05</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>182 rows × 12 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-77160220-716c-41d8-86e6-60e3ddca7208')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-77160220-716c-41d8-86e6-60e3ddca7208 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-77160220-716c-41d8-86e6-60e3ddca7208');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-b90aa30e-4d34-4b39-af1a-05e39b14c862\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-b90aa30e-4d34-4b39-af1a-05e39b14c862')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-b90aa30e-4d34-4b39-af1a-05e39b14c862 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Write to csv"
      ],
      "metadata": {
        "id": "5y-KQYFZAnil"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "nqhKQpH7vlWu"
      },
      "outputs": [],
      "source": [
        "df.to_csv(OUTPUT_PATH, index=False)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "collapsed_sections": [
        "kKrcf_qr-5y4",
        "AOzA4c4m_eyB",
        "of2mkHDe_UrT",
        "lqUeL-r8ADTb",
        "U7vEmPiel3KN"
      ]
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}